{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/burakozturan/bliss/blob/main/BLISS_Lab09_LLMwAPIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Lab 09: LLMs with APIs - Scaling Up Your Research\n",
        "\n",
        "**Duration**: 120 minutes | **Prerequisites**: Lab 08 (LLM Capabilities)\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "By the end of this lab, you'll be able to:\n",
        "- Set up and use API keys securely for philosophical research\n",
        "- Call APIs to analyze philosophical texts\n",
        "- Compare different AI models for your research needs\n",
        "- Engineer effective prompts for better philosophical analysis\n",
        "- Process multiple texts in batches through APIs\n",
        "- Import your own data and apply all API skills\n",
        "\n",
        "## ü§î Why APIs for Philosophy Research?\n",
        "\n",
        "**Simple Analogy**: An API is like ordering at a restaurant\n",
        "- You tell the waiter what you want (your philosophical question)\n",
        "- The waiter takes it to the kitchen (powerful AI model in the cloud)\n",
        "- The kitchen prepares your order (processes your text)\n",
        "- You receive the analysis back\n",
        "\n",
        "| Aspect | Lab 08 (Local Models) | Lab 09 (APIs) |\n",
        "|--------|----------------------|----------------|\n",
        "| **Power** | Limited by your computer | GPT-4, Claude, etc. |\n",
        "| **Cost** | Free | Small cost per use |\n",
        "| **Speed** | Depends on your hardware | Usually faster |\n",
        "| **Privacy** | Completely private | Shared with provider |\n",
        "| **Capability** | Basic analysis | Sophisticated reasoning |\n",
        "\n",
        "**Today's Plan**: Learn each API skill step-by-step, one concept at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1-header"
      },
      "source": [
        "## Part 1: API Key Setup (15 minutes)\n",
        "\n",
        "### üìö What are APIs and API Keys?\n",
        "\n",
        "**API** = Application Programming Interface\n",
        "- A way for your code to \"talk\" to powerful AI models on the internet\n",
        "- Like calling a really smart philosophy expert on the phone\n",
        "\n",
        "**API Key** = Your password to use the API\n",
        "- Proves you're authorized to use the service\n",
        "- Like showing your library card to check out books\n",
        "\n",
        "**Why we need API keys**: They let you access much more powerful AI models than what runs on your computer\n",
        "\n",
        "### üîê Security First\n",
        "**Important**: Never put API keys directly in your code! We'll store them securely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q requests pandas openai huggingface_hub\n",
        "!pip install transformers accelerate datasets sentencepiece -q\n",
        "\n",
        "from huggingface_hub import InferenceClient\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demo1-header"
      },
      "source": [
        "### üîç Demonstration: Setting Up HuggingFace API Key\n",
        "\n",
        "**Follow these steps to get your HuggingFace API key**:\n",
        "\n",
        "1. Go to [huggingface.co](https://huggingface.co)\n",
        "2. Sign up (free account)\n",
        "3. Click your profile ‚Üí Settings ‚Üí Access Tokens\n",
        "4. Create new token ‚Üí Copy it\n",
        "5. In Google Colab: Click üîë (key icon) on left sidebar\n",
        "6. Add secret: Name = `HF_API_KEY`, Value = your token\n",
        "\n",
        "**Watch me set up the secure key loading**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demo1"
      },
      "outputs": [],
      "source": [
        "# üîç DEMONSTRATION: Secure API key loading\n",
        "\n",
        "def setup_huggingface_api():\n",
        "    \"\"\"Securely load HuggingFace API key\"\"\"\n",
        "    try:\n",
        "        hf_api_key = userdata.get('HF_API_KEY')\n",
        "        print(\"‚úÖ HuggingFace API key loaded successfully!\")\n",
        "        print(\"üîê Key is hidden for security\")\n",
        "        return hf_api_key\n",
        "    except:\n",
        "        print(\"‚ùå HuggingFace API key not found.\")\n",
        "        print(\"Please add it using the steps above.\")\n",
        "        return None\n",
        "\n",
        "# Test the setup\n",
        "hf_key = setup_huggingface_api()\n",
        "\n",
        "if hf_key:\n",
        "    print(\"\\nüéâ Great! HuggingFace API is ready to use.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Please set up your HuggingFace API key first.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HF_API_KEY\"] = \"hugginfacekey\"\n",
        "def setup_huggingface_api():\n",
        "    hf_api_key = os.getenv(\"HF_API_KEY\")\n",
        "    if hf_api_key:\n",
        "        print(\"‚úÖ HuggingFace API key loaded successfully!\")\n",
        "        return hf_api_key\n",
        "    else:\n",
        "        print(\"‚ùå HuggingFace API key not found.\")\n",
        "        return None\n",
        "hf_key = setup_huggingface_api()\n",
        "\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()\n"
      ],
      "metadata": {
        "id": "6VycZVNmV8aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise1-header"
      },
      "source": [
        "### üéØ Exercise 1: Set Up OpenRouter API Key\n",
        "\n",
        "**Your Task**: Follow the same process for OpenRouter\n",
        "\n",
        "**Steps for OpenRouter**:\n",
        "1. Go to [openrouter.ai](https://openrouter.ai)\n",
        "2. Sign up (get $1 free credit)\n",
        "3. Dashboard ‚Üí API Keys ‚Üí Create new key\n",
        "4. Copy the key\n",
        "5. In Colab secrets: Add `OPENROUTER_API_KEY`\n",
        "\n",
        "**Now complete this exercise**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise1"
      },
      "outputs": [],
      "source": [
        "# üéØ YOUR TURN: Set up OpenRouter API key\n",
        "\n",
        "def setup_openrouter_api():\n",
        "    \"\"\"TODO: Complete this function to load OpenRouter API key\"\"\"\n",
        "    try:\n",
        "        # TODO: Get the OpenRouter API key from userdata\n",
        "        or_api_key = None  # TODO: Replace None with userdata.get('OPENROUTER_API_KEY')\n",
        "\n",
        "        print(\"‚úÖ OpenRouter API key loaded successfully!\")\n",
        "        print(\"üîê Key is hidden for security\")\n",
        "        return or_api_key\n",
        "    except:\n",
        "        print(\"‚ùå OpenRouter API key not found.\")\n",
        "        return None\n",
        "\n",
        "# TODO: Test your setup and check both keys work\n",
        "or_key = None  # TODO: Call your function\n",
        "\n",
        "# TODO: Check if both keys are working\n",
        "# Complete the if/elif/else logic to check different combinations of hf_key and or_key\n",
        "# Remember: hf_key was created in the demonstration above"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1 Solution\n",
        "\n",
        "def setup_openrouter_api():\n",
        "    \"\"\"Complete this function to load OpenRouter API key\"\"\"\n",
        "    try:\n",
        "        # Get the OpenRouter API key from userdata\n",
        "        or_api_key = userdata.get('OPENROUTER_API_KEY')\n",
        "\n",
        "        print(\"‚úÖ OpenRouter API key loaded successfully!\")\n",
        "        print(\"üîê Key is hidden for security\")\n",
        "        return or_api_key\n",
        "    except:\n",
        "        print(\"‚ùå OpenRouter API key not found.\")\n",
        "        print(\"Please add it using the steps above.\")\n",
        "        return None\n",
        "\n",
        "# Test your setup\n",
        "or_key = setup_openrouter_api()\n",
        "\n",
        "# Check if both keys are working\n",
        "if hf_key and or_key:\n",
        "    print(\"\\nüéâ Excellent! Both API keys are ready.\")\n",
        "    print(\"üöÄ You're ready to use powerful AI models!\")\n",
        "elif hf_key:\n",
        "    print(\"\\n‚ö†Ô∏è HuggingFace ready, but still need OpenRouter key.\")\n",
        "elif or_key:\n",
        "    print(\"\\n‚ö†Ô∏è OpenRouter ready, but still need HuggingFace key.\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Please set up both API keys before continuing.\")\n",
        "\n",
        "print(\"\\nüí° Teaching Note: Students learn secure credential management\")"
      ],
      "metadata": {
        "id": "rV2T2Mt0uplN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2-header"
      },
      "source": [
        "## Part 2: First API Test (15 minutes)\n",
        "\n",
        "### üìö How to Call an API\n",
        "\n",
        "**Basic Process**:\n",
        "1. Send your philosophical question to the API\n",
        "2. API processes it through powerful AI model\n",
        "3. Get back sophisticated analysis\n",
        "\n",
        "**Like Lab 07 pipelines, but much more powerful!**\n",
        "\n",
        "### üîç Demonstration: First HuggingFace API Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demo2"
      },
      "outputs": [],
      "source": [
        "# üîç DEMONSTRATION: First API call to HuggingFace\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "def call_huggingface_api(\n",
        "    text,\n",
        "    model=\"meta-llama/Llama-3.2-1B-Instruct\",   # ‚úÖ tiny model WITH chat support\n",
        "    max_tokens=500\n",
        "):\n",
        "    \"\"\"Call HuggingFace API with philosophical text using chat.completions\"\"\"\n",
        "\n",
        "    if hf_key is None:\n",
        "        return \"‚ùå HuggingFace API key not available\"\n",
        "\n",
        "    try:\n",
        "        # Create HF API client\n",
        "        client = InferenceClient(token=hf_key)\n",
        "\n",
        "        # Send chat completion request\n",
        "        completion = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": text}],\n",
        "            max_tokens=max_tokens,     # ‚úÖ you now control answer length\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        return completion.choices[0].message[\"content\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå HuggingFace Error: {str(e)}\"\n",
        "\n",
        "# Test with a philosophical question\n",
        "philosophical_question = \"What is the difference between knowledge and belief?\"\n",
        "\n",
        "print(\"üß† Testing HuggingFace API...\")\n",
        "print(f\"Question: {philosophical_question}\")\n",
        "print(\"\\nüì° Sending to API...\")\n",
        "\n",
        "response = call_huggingface_api(philosophical_question)\n",
        "\n",
        "print(\"\\n‚úÖ Response received:\")\n",
        "print(f\"{response}\")\n",
        "\n",
        "print(\"\\nüí° Key Insight: The API gave us a sophisticated philosophical analysis!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise2-header"
      },
      "source": [
        "### üéØ Exercise 2: Your First OpenRouter API Call\n",
        "\n",
        "**Your Task**: Create an OpenRouter API function and test it with a different philosophical question\n",
        "\n",
        "**Challenge**: Follow the same pattern as HuggingFace, but for OpenRouter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise2"
      },
      "outputs": [],
      "source": [
        "# üéØ YOUR TURN: Create OpenRouter API function\n",
        "\n",
        "def call_openrouter_api(text, model=\"x-ai/grok-4.1-fast:free\"):\n",
        "    \"\"\" Complete this function to call OpenRouter API\"\"\"\n",
        "    if or_key is None:\n",
        "        return \"‚ùå OpenRouter API key not available\"\n",
        "\n",
        "    try:\n",
        "        # : Create OpenAI client for OpenRouter\n",
        "        client = OpenAI(\n",
        "            base_url=\"https://openrouter.ai/api/v1\",\n",
        "            api_key=or_key,\n",
        "        )\n",
        "\n",
        "        # : Send the philosophical question\n",
        "        completion = client.chat.completions.create(\n",
        "            extra_headers={\n",
        "                \"HTTP-Referer\": \"https://aide-philosophy-research.org\",\n",
        "                \"X-Title\": \"AIDE Philosophy Research\",\n",
        "            },\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": text  # : Use the text parameter\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # : Return the response content\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå OpenRouter Error: {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOp1YBxKK9H7"
      },
      "outputs": [],
      "source": [
        "# TODO: Test with your own philosophical question\n",
        "your_philosophical_question = \"Your question here...\"  # TODO: Change this to your actual philosophical question\n",
        "\n",
        "print(\"üåê Testing OpenRouter API...\")\n",
        "print(f\"Question: {your_philosophical_question}\")\n",
        "print(\"\\nüì° Sending to API...\")\n",
        "\n",
        "# TODO: Call OpenRouter API with your question\n",
        "# Hint: Use the call_openrouter_api() function\n",
        "your_response = None  # TODO: Replace None with the API call\n",
        "\n",
        "print(\"\\n‚úÖ Response received:\")\n",
        "# TODO: Print your response\n",
        "print(\"TODO: Print your response here\")\n",
        "\n",
        "# TODO: Compare with HuggingFace\n",
        "print(\"\\nüîÑ Comparing same question with HuggingFace...\")\n",
        "# TODO: Call HuggingFace API with the same question\n",
        "# Hint: Use the call_huggingface_api() function\n",
        "hf_response = None  # TODO: Replace None with the API call\n",
        "\n",
        "# TODO: Print both responses side by side (first 100 characters)\n",
        "# Hint: Use [:100] to get first 100 characters\n",
        "print(\"TODO: Print HuggingFace preview here\")\n",
        "print(\"TODO: Print OpenRouter preview here\")\n",
        "\n",
        "print(\"\\nüí° Key Insight: Different APIs can give different perspectives on the same question!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3-header"
      },
      "source": [
        "## Part 3: Model Comparison (20 minutes)\n",
        "\n",
        "### üìö Why Compare Different Models?\n",
        "\n",
        "**Different models have different strengths**:\n",
        "- Some are better at reasoning\n",
        "- Some are better at creative thinking  \n",
        "- Some are free, some cost money\n",
        "- Some are faster, some are more thorough\n",
        "\n",
        "**For philosophy research**: You want to find the model that best understands your specific area\n",
        "\n",
        "### üîç Demonstration: Comparing 3 Different Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demo3"
      },
      "outputs": [],
      "source": [
        "# üîç DEMONSTRATION: Compare multiple models on same philosophical problem\n",
        "\n",
        "def compare_models_demo():\n",
        "    \"\"\"Compare 3 different models on the same philosophical question\"\"\"\n",
        "\n",
        "    # The philosophical question to test\n",
        "    test_question = \"Is free will compatible with determinism? Explain briefly.\"\n",
        "\n",
        "    # Models to compare\n",
        "    models_to_test = [\n",
        "        {\n",
        "            'name': 'HuggingFace Phi-4',\n",
        "            'api': 'huggingface',\n",
        "            'model': 'microsoft/phi-4',\n",
        "            'cost': 'Free'\n",
        "        },\n",
        "        {\n",
        "            'name': 'OpenRouter Free',\n",
        "            'api': 'openrouter',\n",
        "            'model': 'moonshotai/kimi-k2:free',\n",
        "            'cost': 'Free'\n",
        "        },\n",
        "        {\n",
        "            'name': 'OpenRouter GPT-3.5',\n",
        "            'api': 'openrouter',\n",
        "            'model': 'openai/gpt-3.5-turbo',\n",
        "            'cost': 'Paid (~$0.002/1K tokens)'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    print(\"üîç MODEL COMPARISON DEMONSTRATION\")\n",
        "    print(f\"Question: {test_question}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for model_info in models_to_test:\n",
        "        print(f\"\\nüß† Testing {model_info['name']} ({model_info['cost']})...\")\n",
        "\n",
        "        # Call the appropriate API\n",
        "        start_time = time.time()\n",
        "        if model_info['api'] == 'huggingface':\n",
        "            response = call_huggingface_api(test_question, model_info['model'])\n",
        "        else:\n",
        "            response = call_openrouter_api(test_question, model_info['model'])\n",
        "\n",
        "        response_time = time.time() - start_time\n",
        "\n",
        "        # Store result\n",
        "        result = {\n",
        "            'model': model_info['name'],\n",
        "            'response': response,\n",
        "            'time': response_time,\n",
        "            'cost': model_info['cost'],\n",
        "            'length': len(response.split()) if isinstance(response, str) else 0\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        # Show preview\n",
        "        print(f\"Time: {response_time:.1f}s | Length: {result['length']} words\")\n",
        "        print(f\"Response: {response[:150]}...\")\n",
        "\n",
        "        time.sleep(2)  # Be nice to APIs\n",
        "\n",
        "    # Summary comparison\n",
        "    print(\"\\nüìä COMPARISON SUMMARY:\")\n",
        "    print(\"-\" * 60)\n",
        "    for result in results:\n",
        "        print(f\"{result['model']:<20} | {result['time']:.1f}s | {result['length']:3d} words | {result['cost']}\")\n",
        "\n",
        "    print(\"\\nüí° Key Observations:\")\n",
        "    print(\"‚Ä¢ Different models give different perspectives\")\n",
        "    print(\"‚Ä¢ Response length and depth vary\")\n",
        "    print(\"‚Ä¢ Speed differences between models\")\n",
        "    print(\"‚Ä¢ Free vs paid models show different capabilities\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the demonstration\n",
        "demo_results = compare_models_demo()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_results"
      ],
      "metadata": {
        "id": "iAzlXe9nx7ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise3-header"
      },
      "source": [
        "### üéØ Exercise 3: Compare Models with Your Philosophical Question\n",
        "\n",
        "**Your Task**: Test the same 3 models with a philosophical question from your own research area\n",
        "\n",
        "**Goal**: Decide which model works best for your type of philosophical analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise3"
      },
      "outputs": [],
      "source": [
        "# üéØ YOUR TURN: Compare models with your research question\n",
        "\n",
        "def your_model_comparison():\n",
        "    \"\"\"TODO: Compare models with your own philosophical question\"\"\"\n",
        "\n",
        "    # TODO: Choose a question from your research area\n",
        "    your_research_question = \"Your question here...\"  # TODO: Replace with your actual question\n",
        "\n",
        "    print(\"üîç YOUR MODEL COMPARISON\")\n",
        "    print(f\"Your Question: {your_research_question}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    models = [\n",
        "        {'name': 'HuggingFace', 'api': 'huggingface'}, # TODO: Replace with your own model\n",
        "        {'name': 'OpenRouter Free', 'api': 'openrouter', 'model': 'moonshotai/kimi-k2:free'}, # TODO: Replace with your own model\n",
        "        {'name': 'OpenRouter GPT-3.5', 'api': 'openrouter', 'model': 'openai/gpt-3.5-turbo'} # TODO: Replace with your own model\n",
        "    ]\n",
        "\n",
        "    your_results = []\n",
        "\n",
        "    for i, model_info in enumerate(models):\n",
        "        print(f\"\\nüß† Testing {model_info['name']}...\")\n",
        "\n",
        "        # TODO: Time the response\n",
        "        start_time = None  # TODO: Get current time using time.time()\n",
        "\n",
        "        # TODO: Call the appropriate API based on model_info['api']\n",
        "        if None:  # TODO: Check if api is 'huggingface'\n",
        "            response = None  # TODO: Call HuggingFace API\n",
        "        else:\n",
        "            response = None  # TODO: Call OpenRouter API with model\n",
        "\n",
        "        # TODO: Calculate response time\n",
        "        response_time = None  # TODO: Current time minus start_time\n",
        "\n",
        "        # TODO: Store the result in a dictionary\n",
        "        result = {\n",
        "            # TODO: Fill in the dictionary with model name, response, time, and word count\n",
        "        }\n",
        "        your_results.append(result)\n",
        "\n",
        "        # TODO: Show preview (time, word count, first 100 characters)\n",
        "        print(\"TODO: Print time and length\")\n",
        "        print(\"TODO: Print response preview\")\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "    # TODO: Print results summary table\n",
        "    print(\"\\nüìä YOUR RESULTS:\")\n",
        "    for result in your_results:\n",
        "        # TODO: Print formatted results for each model\n",
        "        print(\"TODO: Print model comparison line\")\n",
        "\n",
        "    return your_results\n",
        "\n",
        "# TODO: Run your comparison\n",
        "my_results = None  # TODO: Call your function\n",
        "\n",
        "# TODO: Decide which model you prefer\n",
        "print(\"\\nüéØ My Decision:\")\n",
        "print(\"For my research, I choose: _____ because _____\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part4-header"
      },
      "source": [
        "## Part 4: Prompt Engineering (25 minutes)\n",
        "\n",
        "### üìö What is Prompt Engineering?\n",
        "\n",
        "**Prompt Engineering** = The art of asking AI the right questions in the right way\n",
        "\n",
        "**Why it matters**: The same AI model can give vastly different results based on how you ask\n",
        "\n",
        "**Philosophy connection**: Like Socratic questioning - how you ask determines what you discover\n",
        "\n",
        "**Basic Pattern**:\n",
        "```\n",
        "You are a [role]\n",
        "Your task: [clear instruction]\n",
        "Format: [how to structure the answer]\n",
        "```\n",
        "\n",
        "### üîç Demonstration: Weak vs Strong Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demo4"
      },
      "outputs": [],
      "source": [
        "# üîç DEMONSTRATION: Prompt engineering with philosophical argument\n",
        "\n",
        "def prompt_engineering_demo():\n",
        "    \"\"\"Show the power of good prompt design\"\"\"\n",
        "\n",
        "    # Philosophical argument to analyze\n",
        "    argument = \"If we have free will, then we are morally responsible for our actions. But if determinism is true, then we don't have free will. Therefore, if determinism is true, we are not morally responsible.\"\n",
        "\n",
        "    print(\"üîç PROMPT ENGINEERING DEMONSTRATION\")\n",
        "    print(f\"Argument: {argument}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # üö´ WEAK PROMPT\n",
        "    weak_prompt = f\"What do you think about this argument? {argument}\"\n",
        "\n",
        "    print(\"\\nüö´ WEAK PROMPT:\")\n",
        "    print(f\"'{weak_prompt}'\")\n",
        "    print(\"\\nüì° Sending to API...\")\n",
        "\n",
        "    weak_response = call_openrouter_api(weak_prompt)\n",
        "    print(f\"\\nWeak Response: {weak_response}\")\n",
        "\n",
        "    time.sleep(3)\n",
        "\n",
        "    # ‚úÖ STRONG PROMPT\n",
        "    strong_prompt = f\"\"\"You are a philosophy professor analyzing logical arguments.\n",
        "\n",
        "Your task: Analyze this philosophical argument for logical structure and validity.\n",
        "\n",
        "Argument: {argument}\n",
        "\n",
        "Format your response as:\n",
        "1. Premise identification (list each premise clearly)\n",
        "2. Conclusion identification\n",
        "3. Logical structure (valid/invalid and why)\n",
        "4. One potential objection\n",
        "\n",
        "Keep each section concise but thorough (max 50 words per section).\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"\\n‚úÖ STRONG PROMPT:\")\n",
        "    print(f\"'{strong_prompt}'\")\n",
        "    print(\"\\nüì° Sending to API...\")\n",
        "\n",
        "    strong_response = call_openrouter_api(strong_prompt)\n",
        "    print(f\"\\nStrong Response: {strong_response}\")\n",
        "\n",
        "    # Comparison\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"\\nüìä COMPARISON:\")\n",
        "    print(f\"Weak response length: {len(weak_response.split())} words\")\n",
        "    print(f\"Strong response length: {len(strong_response.split())} words\")\n",
        "\n",
        "    print(\"\\nüéØ Key Improvements in Strong Prompt:\")\n",
        "    print(\"‚úÖ Clear role definition (philosophy professor)\")\n",
        "    print(\"‚úÖ Specific task description (analyze for logical structure)\")\n",
        "    print(\"‚úÖ Structured output format (numbered sections)\")\n",
        "    print(\"‚úÖ Length constraints (max 50 words per section)\")\n",
        "    print(\"‚úÖ Specific requirements (premises, conclusion, validity, objection)\")\n",
        "\n",
        "    return weak_response, strong_response\n",
        "\n",
        "# Run the demonstration\n",
        "weak_demo, strong_demo = prompt_engineering_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise4-header"
      },
      "source": [
        "### üéØ Exercise 4: Improve a Weak Prompt\n",
        "\n",
        "**Your Task**: Take a weak prompt and transform it into a strong one for philosophical analysis\n",
        "\n",
        "**Challenge**: Apply the principles you learned to create better philosophical analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise4"
      },
      "outputs": [],
      "source": [
        "# üéØ YOUR TURN: Transform weak prompt into strong prompt\n",
        "\n",
        "def improve_philosophical_prompt():\n",
        "    \"\"\"TODO: Improve this weak prompt for better philosophical analysis\"\"\"\n",
        "\n",
        "    quote = \"The unexamined life is not worth living.\" # Socrates\n",
        "    weak_prompt = f\"Tell me about this quote: {quote}\"\n",
        "\n",
        "    print(\"üîß PROMPT IMPROVEMENT EXERCISE\")\n",
        "    print(f\"Quote: {quote}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    print(f\"\\nüö´ WEAK PROMPT: '{weak_prompt}'\")\n",
        "\n",
        "    # TODO: Create your improved prompt using: Role + Task + Format + Constraints\n",
        "    improved_prompt = f\"\"\"TODO: Write your improved prompt here.\n",
        "\n",
        "Include:\n",
        "- Clear role (e.g., \"You are a philosophy scholar...\")\n",
        "- Specific task (e.g., \"Analyze this quote...\")\n",
        "- Structured format (e.g., \"Format as: 1. X, 2. Y...\")\n",
        "- Constraints (e.g., \"Under 150 words, scholarly style\")\n",
        "\n",
        "Your improved prompt: {quote}\"\"\"\n",
        "\n",
        "    print(f\"\\n‚úÖ YOUR IMPROVED PROMPT: '{improved_prompt}'\")\n",
        "\n",
        "    # TODO: Test both prompts and compare results\n",
        "    print(\"\\nüîÑ Testing both prompts...\")\n",
        "\n",
        "    # TODO: Test weak prompt\n",
        "    weak_response = None  # TODO: Call API with weak_prompt\n",
        "    print(f\"Weak Result: {weak_response[:100]}...\")\n",
        "\n",
        "    time.sleep(3)\n",
        "\n",
        "    # TODO: Test improved prompt\n",
        "    improved_response = None  # TODO: Call API with improved_prompt\n",
        "    print(f\"Improved Result: {improved_response[:100]}...\")\n",
        "\n",
        "    # TODO: Compare word counts\n",
        "    print(f\"\\nWeak: ___ words | Improved: ___ words\")\n",
        "\n",
        "    return weak_response, improved_response\n",
        "\n",
        "# TODO: Run your exercise\n",
        "weak_result, improved_result = None  # TODO: Call your function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part5-header"
      },
      "source": [
        "## Part 5: Batch Processing (25 minutes)\n",
        "\n",
        "### üìö What is Batch Processing?\n",
        "\n",
        "**Batch Processing** = Analyzing multiple texts systematically, one after another\n",
        "\n",
        "**Why you need it**:\n",
        "- Analyze 10, 50, or 100+ philosophical texts\n",
        "- Consistent analysis across all texts\n",
        "- Save time compared to manual analysis\n",
        "- Keep organized records of all results\n",
        "\n",
        "**Like**: Grading a stack of philosophy papers with the same rubric\n",
        "\n",
        "### üîç Demonstration: Batch Process 3 Philosophical Quotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demo5"
      },
      "outputs": [],
      "source": [
        "# üîç DEMONSTRATION: Batch processing philosophical quotes\n",
        "\n",
        "def batch_processing_demo():\n",
        "    \"\"\"Process multiple philosophical quotes systematically\"\"\"\n",
        "\n",
        "    # Sample philosophical quotes to process\n",
        "    philosophical_quotes = [\n",
        "        {\n",
        "            'id': 1,\n",
        "            'philosopher': 'Aristotle',\n",
        "            'quote': 'The good life is one inspired by love and guided by knowledge.',\n",
        "            'period': 'Ancient'\n",
        "        },\n",
        "        {\n",
        "            'id': 2,\n",
        "            'philosopher': 'Kant',\n",
        "            'quote': 'Act only according to that maxim whereby you can at the same time will that it should become a universal law.',\n",
        "            'period': 'Modern'\n",
        "        },\n",
        "        {\n",
        "            'id': 3,\n",
        "            'philosopher': 'Rawls',\n",
        "            'quote': 'Justice is the first virtue of social institutions.',\n",
        "            'period': 'Contemporary'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    print(\"üîç BATCH PROCESSING DEMONSTRATION\")\n",
        "    print(f\"Processing {len(philosophical_quotes)} philosophical quotes...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create our analysis template\n",
        "    analysis_template = \"\"\"You are a philosophy professor analyzing philosophical quotes.\n",
        "\n",
        "Analyze this quote by {philosopher} ({period} period): \"{quote}\"\n",
        "\n",
        "Provide:\n",
        "1. Main philosophical concept (1-2 sentences)\n",
        "2. Ethical framework or school (1-2 sentences)\n",
        "3. Contemporary relevance (1-2 sentences)\n",
        "\n",
        "Keep response under 80 words, scholarly but accessible.\"\"\"\n",
        "\n",
        "    # Process each quote\n",
        "    batch_results = []\n",
        "\n",
        "    for i, quote_data in enumerate(philosophical_quotes):\n",
        "        print(f\"\\nüìù Processing {i+1}/{len(philosophical_quotes)}: {quote_data['philosopher']}...\")\n",
        "\n",
        "        # Create the specific prompt for this quote\n",
        "        formatted_prompt = analysis_template.format(\n",
        "            philosopher=quote_data['philosopher'],\n",
        "            period=quote_data['period'],\n",
        "            quote=quote_data['quote']\n",
        "        )\n",
        "\n",
        "        # Send to API\n",
        "        start_time = time.time()\n",
        "        analysis = call_openrouter_api(formatted_prompt)\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        # Store the result\n",
        "        result = {\n",
        "            'id': quote_data['id'],\n",
        "            'philosopher': quote_data['philosopher'],\n",
        "            'original_quote': quote_data['quote'],\n",
        "            'period': quote_data['period'],\n",
        "            'analysis': analysis,\n",
        "            'processing_time': processing_time,\n",
        "            'success': not analysis.startswith('‚ùå')\n",
        "        }\n",
        "\n",
        "        batch_results.append(result)\n",
        "\n",
        "        # Show progress\n",
        "        if result['success']:\n",
        "            print(f\"‚úÖ Success ({processing_time:.1f}s): {analysis[:60]}...\")\n",
        "        else:\n",
        "            print(f\"‚ùå Failed: {analysis}\")\n",
        "\n",
        "        # Rate limiting - be nice to APIs\n",
        "        time.sleep(2)\n",
        "\n",
        "    # Create summary\n",
        "    print(\"\\nüìä BATCH PROCESSING SUMMARY:\")\n",
        "    print(\"=\" * 60)\n",
        "    successful = sum(1 for r in batch_results if r['success'])\n",
        "    total = len(batch_results)\n",
        "    avg_time = sum(r['processing_time'] for r in batch_results if r['success']) / successful if successful > 0 else 0\n",
        "\n",
        "    print(f\"Successfully processed: {successful}/{total} ({successful/total*100:.1f}%)\")\n",
        "    print(f\"Average processing time: {avg_time:.2f} seconds\")\n",
        "    print(f\"Total time: {sum(r['processing_time'] for r in batch_results):.1f} seconds\")\n",
        "\n",
        "    # Show a sample result\n",
        "    if successful > 0:\n",
        "        sample = batch_results[0]\n",
        "        print(f\"\\nüìù Sample Analysis:\")\n",
        "        print(f\"Philosopher: {sample['philosopher']}\")\n",
        "        print(f\"Quote: {sample['original_quote'][:50]}...\")\n",
        "        print(f\"Analysis: {sample['analysis']}\")\n",
        "\n",
        "    # Save results to CSV\n",
        "    import pandas as pd\n",
        "    results_df = pd.DataFrame(batch_results)\n",
        "    results_df.to_csv('batch_philosophy_analysis.csv', index=False)\n",
        "    print(f\"\\nüíæ Results saved to 'batch_philosophy_analysis.csv'\")\n",
        "\n",
        "    print(\"\\nüí° Key Benefits of Batch Processing:\")\n",
        "    print(\"‚úÖ Consistent analysis across all texts\")\n",
        "    print(\"‚úÖ Organized results with all data preserved\")\n",
        "    print(\"‚úÖ Efficient processing of multiple texts\")\n",
        "    print(\"‚úÖ Easy to export and analyze results\")\n",
        "\n",
        "    return batch_results\n",
        "\n",
        "# Run the demonstration\n",
        "demo_batch_results = batch_processing_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise5-header"
      },
      "source": [
        "### üéØ Exercise 5: Batch Process Your Own Philosophical Texts\n",
        "\n",
        "**Your Task**: Create your own set of philosophical texts and batch process them\n",
        "\n",
        "**Goal**: Apply batch processing to texts relevant to your research area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise5"
      },
      "outputs": [],
      "source": [
        "# üéØ YOUR TURN: Batch process your philosophical texts\n",
        "\n",
        "def your_batch_processing():\n",
        "    \"\"\"TODO: Create and process your own philosophical texts\"\"\"\n",
        "\n",
        "    # TODO: Replace with your actual philosophical texts (4-5 texts)\n",
        "    your_philosophical_texts = [\n",
        "        {\n",
        "            'id': 1,\n",
        "            'source': 'Your Source 1',  # TODO: Real source name\n",
        "            'text': 'Your philosophical text here...',  # TODO: Real text/quote\n",
        "            'topic': 'ethics',  # TODO: Your topic\n",
        "            'relevance': 'High'\n",
        "        },\n",
        "        # TODO: Add 3-4 more texts following the same structure\n",
        "    ]\n",
        "\n",
        "    print(f\"üéØ Processing {len(your_philosophical_texts)} texts...\")\n",
        "\n",
        "    # TODO: Create your analysis template\n",
        "    your_analysis_template = \"\"\"You are a philosophy researcher.\n",
        "\n",
        "Analyze this text from {source} (topic: {topic}): \"{text}\"\n",
        "\n",
        "Provide:\n",
        "1. Main claim: (2-3 sentences)\n",
        "2. Approach: (1-2 sentences)\n",
        "3. Research relevance: (2-3 sentences)\n",
        "\n",
        "Under 100 words.\"\"\"\n",
        "\n",
        "    your_results = []\n",
        "\n",
        "    # TODO: Process each text\n",
        "    for i, text_data in enumerate(your_philosophical_texts):\n",
        "        print(f\"\\nüìù Processing {i+1}: {text_data['source']}...\")\n",
        "\n",
        "        # TODO: Format prompt with text data\n",
        "        formatted_prompt = None  # TODO: Use .format() to fill template\n",
        "\n",
        "        # TODO: Send to API\n",
        "        analysis = None  # TODO: Call API with formatted_prompt\n",
        "\n",
        "        # TODO: Store result\n",
        "        result = {\n",
        "            'source': text_data['source'],\n",
        "            'topic': text_data['topic'],\n",
        "            'analysis': analysis,\n",
        "            'success': not analysis.startswith('‚ùå')\n",
        "        }\n",
        "        your_results.append(result)\n",
        "\n",
        "        # TODO: Show progress\n",
        "        print(f\"‚úÖ {analysis[:50]}...\" if result['success'] else f\"‚ùå Failed\")\n",
        "        time.sleep(2)\n",
        "\n",
        "    # TODO: Print summary\n",
        "    successful = sum(1 for r in your_results if r['success'])\n",
        "    print(f\"\\nüìä Processed: {successful}/{len(your_results)} texts\")\n",
        "\n",
        "    return your_results\n",
        "\n",
        "# TODO: Run your batch processing\n",
        "my_results = None  # TODO: Call your function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part6-header"
      },
      "source": [
        "Part 6: Real Data Upload (20 minutes) - BONUS\n",
        "üìö Working with Real Research Data\n",
        "Real research scenario: You have a CSV file with philosophical texts to analyze\n",
        "* Paper abstracts from your literature review\n",
        "* Quotes from primary sources\n",
        "* Survey responses about ethical dilemmas\n",
        "Today: Learn to import real data and apply all your API skills\n",
        "üéØ Exercise 6: Upload and Process Your Real Data\n",
        "Your Mission: Apply ALL your API skills to real philosophical data\n",
        "Skills to Apply:\n",
        "* ‚úÖ API calls (Parts 1-2)\n",
        "* ‚úÖ Model comparison (Part 3)\n",
        "* ‚úÖ Prompt engineering (Part 4)\n",
        "* ‚úÖ Batch processing (Part 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMFk32ieK9IC"
      },
      "outputs": [],
      "source": [
        "# üéØ YOUR TURN: Complete research pipeline\n",
        "\n",
        "def your_complete_pipeline():\n",
        "    \"\"\"TODO: Apply all API skills to your data\"\"\"\n",
        "\n",
        "    print(\"üéØ YOUR COMPLETE RESEARCH PIPELINE\")\n",
        "    # Upload your own CSV file (recommended)\n",
        "    # STEP 1: Import your data\n",
        "    # Option A: Upload CSV file\n",
        "    # from google.colab import files\n",
        "    # uploaded = files.upload()\n",
        "    # your_df = pd.read_csv(list(uploaded.keys())[0])\n",
        "\n",
        "    print(f\"üìä Data loaded: {your_df.shape}\")\n",
        "\n",
        "    # STEP 2: Apply your skills\n",
        "    # TODO: pick a model (from Part 3)\n",
        "    # TODO: Use prompt engineering (from Part 4)\n",
        "    # TODO: Use batch processing (from Part 5)\n",
        "\n",
        "    # STEP 3: Save enriched results\n",
        "    # TODO: Save your results to CSV\n",
        "\n",
        "    return your_df\n",
        "\n",
        "# TODO: Run your complete pipeline\n",
        "my_pipeline_results = your_complete_pipeline()\n",
        "\n",
        "print(\"\\nüéâ BONUS COMPLETE!\")\n",
        "print(\"You've applied all API skills to real philosophical data!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrap-up-header"
      },
      "source": [
        "## Lab 09 Summary & Next Steps\n",
        "\n",
        "### üéâ What You've Accomplished\n",
        "\n",
        "‚úÖ **API Fundamentals**: Set up secure access to HuggingFace and OpenRouter APIs\n",
        "‚úÖ **Model Comparison**: Compared free vs paid models for philosophical research\n",
        "‚úÖ **Prompt Engineering**: Learned to design effective prompts for better analysis\n",
        "‚úÖ **Data Processing**: Imported data and batch processed it through APIs\n",
        "\n",
        "### üõ†Ô∏è Your New Research Toolkit\n",
        "\n",
        "You now have working code for:\n",
        "- Secure API access and key management\n",
        "- Systematic model comparison\n",
        "- Structured prompt templates for philosophical analysis\n",
        "- Batch processing pipelines for multiple texts\n",
        "\n",
        "### üîÑ From Lab 08 to Lab 09: Your Progress\n",
        "\n",
        "| Capability | Lab 08 (Local) | Lab 09 (APIs) |\n",
        "|------------|----------------|----------------|\n",
        "| **Analysis Power** | Basic classification, simple Q&A | Sophisticated reasoning, nuanced analysis |\n",
        "| **Flexibility** | Pre-trained tasks | Custom prompts for any philosophical task |\n",
        "| **Scale** | Single texts, limited by hardware | Batch processing, cloud-scale analysis |\n",
        "| **Cost** | Free | Strategic: free for exploration, paid for final work |\n",
        "\n",
        "### üöÄ Preview: Lab 10 - RAG Systems\n",
        "\n",
        "**Next week**: Combine your API skills with your philosophical library\n",
        "\n",
        "**RAG (Retrieval-Augmented Generation)** will let you:\n",
        "- Ask questions across your entire dissertation corpus\n",
        "- Get answers with specific citations from your sources\n",
        "- Find connections between different philosophical texts\n",
        "- Build a \"research assistant\" that knows your specific materials\n",
        "\n",
        "**Example**: \"What do Kant and Rawls have in common regarding justice?\" ‚Üí Get answer with exact quotes and page numbers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "## üÜò Troubleshooting Guide\n",
        "\n",
        "| Issue | Likely Cause | Solution |\n",
        "|-------|--------------|----------|\n",
        "| **API key not working** | Incorrect key or format | Regenerate key, check for extra spaces |\n",
        "| **\"Rate limit exceeded\"** | Too many requests too fast | Add longer `time.sleep()` between calls |\n",
        "| **\"Model not found\"** | Wrong model name | Check exact model names on provider website |\n",
        "| **High unexpected costs** | Using expensive model by mistake | Double-check model names, use free models first |\n",
        "| **Poor analysis results** | Weak prompt design | Improve prompt structure, add examples |\n",
        "| **Batch processing fails** | Data format issues | Check your CSV columns, handle missing data |\n",
        "| **\"JSON decode error\"** | API response issues | Check API status, try simpler prompts |\n",
        "| **Import errors** | Wrong file path/format | Verify file location, check pandas documentation |\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}